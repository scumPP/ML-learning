# 朴素贝叶斯

***

## 主要思想

主要用于分类，是基于贝叶斯估计和特征向量独立性假设的**生成模型**
朴素贝叶斯模型根据训练集，首先学习先验概率分布$P(Y=c_k),k=1,2,...,K$,然后学习到条件概率分布$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k),k=1,2,...,K$,于是可以学习到联合概率分布$P(X,Y)$
由于朴素贝叶斯对条件概率做了条件独立性的假设，即：
$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)=\displaystyle\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)\tag{1}$$
朴素贝叶斯分类时，将输入变量后的最大后验概率的类作为输出。这里看不懂没关系，后面会有解释，公式为：
$$P(Y=c_k|X=x)=\cfrac{P(X=x|Y=c_k)P(Y=c_k)}{\displaystyle\sum_{k}P(X=x|Y=c_k)P(Y=c_k)}\tag{2}$$
将公式（1）带入公式（2）中，得到：
$$P(Y=c_k|X=x)=\cfrac{P(X=x|Y=c_k)P(Y=c_k)}{\displaystyle\sum_{k}P(Y=c_k)\displaystyle\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_k)}k=1,2,...,K\tag{3}$$
公式（3）就是朴素贝叶斯的基本公式，那么朴素贝叶斯的分类器可表示为:
$$y=f(x)=\argmax\limits_{c_k}\cfrac{P(X=x|Y=c_k)P(Y=c_k)}{\displaystyle\sum_{k}P(Y=c_k)\displaystyle\prod_{j}P(X^{(j)}=x^{(j)}|Y=c_k)}k=1,2,...,K\tag{3}$$
这里根据最大后验概率（后面讲）进行变换就为：
$$y=\argmax\limits_{c_k}P(Y=c_k)\displaystyle\prod_{(j=1)}^nP(X^{(j)}=x{(j)}|Y=c_k)\tag{4}$$
在这里，对公式（2）做一下解释，公式（2）主要来源于贝叶斯公式，那么我们就从贝叶斯估计开始介绍，在统计学中，一直存有两个学派：频率派和贝叶斯派。这里的贝叶斯定理就属于贝叶斯派里的经典。
首先，我们先了解下，什么是先验概率、后验概率，我们知道$P(A|B)$表示的是在B发生的情况下A的可能性：
首先，在事件B发生之前，我们对事件A有一个基本的判断，这就称为A的先验概率（边缘概率），记作$P(A)$
其次，事件B发生之后，我们对事件A的发生概率要重新评估，称为A的后验概率（条件概率），记作$P(A|B)$

我们现在了解了贝叶斯里的基本概念，再来看看贝叶斯定理的公式：
$$P(B_i|A)=\cfrac{P(B_i)P(A|B_i)}{\displaystyle\sum_{j=1}^nP(B_i)P(A|B_j)}
$$
那么这个定理是如何推导出来的呢？其实很简单，就是由基本的条件概率公式推导而来：
$P(A|B)=\cfrac{P(AB)}{P(B)}$&emsp;和&emsp;$P(B|A)=\cfrac{P(AB)}{P(A)}$
以上是两个条件概率公式，我们将$P(AB)$替换掉，就得到这样的公式：
$$P(B|A)=\cfrac{P(B)P(A|B)}{P(A)}$$
我们知道全概率公式为
$$P(A)=P(A|B_1)+P(A|B_2)+...+P(A|B_n)$$
因此，将全概率公式带入后就得到了贝叶斯定理啦！

上面的公式（4）的由来是根据最大化后验概率，那么最大化后验概率的思想是如何来的呢？
我们知道，如果想让一个模型的性能达到最好，需要使风险函数最小。这里我们使用的是期望风险函数，因为朴素贝叶斯模型可以求出联合概率分布，所以直接使用期望风险函数即可。
由于贝叶斯模型是解决分类问题，因此损失函数使用的是0-1损失函数：
$$L(Y,f(X))=\begin{cases}
   1 ,\quad Y \neq f(X) \\
   0 ,\quad Y=f(X)
   \end{cases}$$
期望风险函数为：
$$R_{exp}(f)=E[L(Y,f(X))]$$
具体的推导过程看图片：
![the](./assets/1.jpg 'lala')
![the](./assets/2.jpg 'lala')
根据图片，我们最终推导出了朴素贝叶斯模型的公式：
$$y=\argmax\limits_{c_k}P(Y=c_k)\displaystyle\prod_{(j=1)}^nP(X^{(j)}=x{(j)}|Y=c_k)\tag{4}$$

## 算法过程

朴素贝叶斯的具体算法过程分为两种：极大似然估计和贝叶斯估计
首先，先来看算法的输入输出：

+ 输入：训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$,其中$x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(j)})^T$,$x_i^{(j)}$是第$i$个特征，$x_i^{(j)} \in \{a_{j1},a_{j2},..,a_{jS_j}\}$,$a_{jl}$是第$j$个特征可能取的第$l$个值，$j=1,2,...,n,l=1,2,..,S_j,y_i \in \{c_1,c_2,...,c_K\}$;实例$x$
+ 输出：实例$x$的分类

1. 极大似然估计方法

(1) 计算先验概率和条件概率
$$P(Y=c_k)=\cfrac{\displaystyle\sum_{i=1}^NI(y_i=c_k)}{N},\quad k=1,2,...,K$$
$$P(X^{j}=a_{jl}|Y=c_k)=\cfrac{\displaystyle\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\displaystyle\sum_{i=1}^NI(y_i=c_k)} \quad \quad j=1,2,...,n;\quad l=1,2,...,S_j;\quad k=1,2,...,K$$
(2) 对于给定的实例$x=(x^{(1)},x^{(2)},...,x^{(n)})^T$，计算
$$P(Y=c_k)\displaystyle\prod_{j=1}^nP(X^{(j)}=x^{(n)}|Y=c_k),\quad k=1,2,...,K$$
(3) 确定实例$x$的类
$$y=\argmax\limits_{c_k}P(Y=c_k)\displaystyle\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)$$

2. 贝叶斯估计方法

由于极大似然估计会出现所要顾及的概率值为0的情况，这时候计算结果就会影响到后验概率，时分类产生偏差，采用贝叶斯估计可以解决这个问题。
我们只要在极大似然估计的基础上加个常数$\lambda$,具体为：
(1) 先验概率为：
$$P_\lambda(Y=c_k)=\cfrac{\displaystyle\sum_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda}$$
(2) 条件概率为：
$$P_\lambda(X^{(j)}=a_{jl}|Y=c_k)=\cfrac{\displaystyle\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\displaystyle\sum_{i=1}^NI(y_i=c_k)+S_j\lambda}$$

在以上公式中，$\lambda\geq0$ 。$\lambda=1$时，称为拉普拉斯平滑；$\lambda=0$ 时，此时就是极大似然估计。

## 总 结

1. 朴素贝叶斯是通过求联合概率分布进而求得后验概率的，因此它是典型的生成模型。其中概率估计有两种方法：极大似然估计和贝叶斯估计。
2. “朴素”主要体现在此模型的一个假设：输入变量之间条件独立，由于这个假设，使得模型的学习和预测大为简化，也易于实现，但是分类的性能不一定很好。
3. 朴素贝叶斯的损失函数是0-1损失函数，学习的核心思想是最大化后验概率。
